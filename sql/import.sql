-- ============================================================================
-- Document Crawler Data Import Script
-- Import CSV data into PostgreSQL database
-- ============================================================================
-- 
-- This script imports data from the CSV file generated by the crawler
-- into the PostgreSQL database.
--
-- Prerequisites:
--   1. Run schema.sql first to create the table structure
--   2. Ensure the CSV file exists at the specified path
--   3. PostgreSQL user has read permissions on the CSV file
--
-- Usage:
--   psql -U postgres -d document_index -f import.sql
-- 
-- Or use the Python import script:
--   python import_to_db.py
--
-- ============================================================================

-- ============================================================================
-- Method 1: Using COPY command (requires superuser or file access)
-- ============================================================================

-- Note: COPY command requires the file to be accessible from the PostgreSQL server
-- If running locally, use the absolute path to the CSV file

-- Clear existing data (optional - comment out if you want to keep existing data)
-- TRUNCATE TABLE documents RESTART IDENTITY CASCADE;

-- Import from CSV file
-- Replace '/path/to/output/extracted_data.csv' with the actual path
/*
COPY documents (file_path, file_name, file_type, file_size, content, archive_path, created_date, content_hash)
FROM '/path/to/output/extracted_data.csv'
WITH (
    FORMAT CSV,
    HEADER TRUE,
    DELIMITER ',',
    ENCODING 'UTF8',
    NULL ''
);
*/

-- ============================================================================
-- Method 2: Using pg_read_file (alternative)
-- ============================================================================

-- This method reads the file as text and parses it
/*
INSERT INTO documents (file_path, file_name, file_type, file_size, content, archive_path, created_date, content_hash)
SELECT 
    split_part(line, ',', 2) AS file_path,
    split_part(line, ',', 3) AS file_name,
    split_part(line, ',', 4) AS file_type,
    split_part(line, ',', 5)::BIGINT AS file_size,
    split_part(line, ',', 6) AS content,
    split_part(line, ',', 7) AS archive_path,
    split_part(line, ',', 8)::TIMESTAMP AS created_date,
    split_part(line, ',', 9) AS content_hash
FROM (
    SELECT unnest(string_to_array(pg_read_file('/path/to/output/extracted_data.csv'), E'\n')) AS line
) AS lines
WHERE line != '' AND line NOT LIKE 'id,file_path%';
*/

-- ============================================================================
-- Verify import
-- ============================================================================

-- Count imported records
SELECT COUNT(*) AS total_documents FROM documents;

-- Count by file type
SELECT file_type, COUNT(*) AS count 
FROM documents 
GROUP BY file_type 
ORDER BY count DESC;

-- Show sample records
SELECT id, file_name, file_type, LEFT(content, 100) AS content_preview
FROM documents
LIMIT 5;

-- ============================================================================
-- Note: For complex CSV with quoted fields, use Python import script
-- ============================================================================

-- The Python script (import_to_db.py) handles:
-- - Proper CSV parsing with quoted fields
-- - Connection management
-- - Error handling
-- - Batch inserts for large datasets