#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Database Import Script - Imports CSV data into PostgreSQL
================================================================================
This script imports the CSV data generated by the crawler into PostgreSQL
database for full-text search.

Usage:
    python import_to_db.py [--csv path/to/extracted_data.csv] [--db document_index]

Requirements:
    pip install psycopg2-binary
"""

import os
import sys
import csv
import argparse
from datetime import datetime


def create_database_if_not_exists(db_config: dict) -> bool:
    """Create database if it doesn't exist."""
    conn_config = db_config.copy()
    conn_config['database'] = 'postgres'

    try:
        conn = psycopg2.connect(**conn_config)
        conn.autocommit = True
        cursor = conn.cursor()

        cursor.execute("SELECT 1 FROM pg_database WHERE datname = %s", (db_config['database'],))
        exists = cursor.fetchone()

        if not exists:
            db_name = psycopg2.extensions.quote_ident(db_config['database'], conn)
            print(f"Creating database: {db_config['database']}")
            cursor.execute(f"CREATE DATABASE {db_name}")
            print("Database created successfully!")
        else:
            print(f"Database {db_config['database']} already exists")

        cursor.close()
        conn.close()
        return True

    except psycopg2.Error as e:
        print(f"Error creating database: {e}")
        return False

try:
    import psycopg2
    from psycopg2.extras import execute_batch
except ImportError:
    print("Error: psycopg2 is not installed.")
    print("Install it with: pip install psycopg2-binary")
    sys.exit(1)


# Database configuration
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'document_index',
    'user': 'postgres',
    'password': 'postgres'
}

# SQL queries
CREATE_TABLE_SQL = """CREATE TABLE IF NOT EXISTS documents (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    file_name TEXT NOT NULL,
    file_type VARCHAR(20),
    file_size BIGINT,
    content TEXT,
    archive_path TEXT,
    created_date TIMESTAMP,
    indexed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    content_hash VARCHAR(32),
    search_vector tsvector
);"""

CREATE_INDEXES_SQL = """
CREATE INDEX IF NOT EXISTS idx_documents_file_type ON documents(file_type);
CREATE INDEX IF NOT EXISTS idx_documents_file_name ON documents(file_name);
CREATE INDEX IF NOT EXISTS idx_documents_content_hash ON documents(content_hash);
CREATE INDEX IF NOT EXISTS idx_documents_search_vector ON documents USING GIN(search_vector);
"""

CREATE_TRIGGER_FUNCTION_SQL = """
CREATE OR REPLACE FUNCTION update_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector :=
        setweight(to_tsvector('russian', COALESCE(NEW.file_name, '')), 'A') ||
        setweight(to_tsvector('russian', COALESCE(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
"""

CREATE_TRIGGER_SQL = """
DROP TRIGGER IF EXISTS documents_search_vector_update ON documents;
CREATE TRIGGER documents_search_vector_update
    BEFORE INSERT OR UPDATE ON documents
    FOR EACH ROW
    EXECUTE FUNCTION update_search_vector();
"""

INSERT_DOCUMENT_SQL = """
INSERT INTO documents (file_path, file_name, file_type, file_size, content, archive_path, created_date, content_hash)
VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
RETURNING id;
"""


def create_connection(db_config: dict):
    """Create a connection to PostgreSQL database."""
    try:
        conn = psycopg2.connect(**db_config)
        print(f"Connected to database: {db_config['database']}")
        return conn
    except psycopg2.Error as e:
        print(f"Error connecting to database: {e}")
        return None


def setup_database(conn):
    """Set up database schema."""
    cursor = conn.cursor()
    
    print("Setting up database schema...")
    
    cursor.execute(CREATE_TABLE_SQL)
    cursor.execute(CREATE_INDEXES_SQL)
    cursor.execute(CREATE_TRIGGER_FUNCTION_SQL)
    cursor.execute(CREATE_TRIGGER_SQL)
    
    conn.commit()
    print("Database setup complete!")


def parse_datetime(dt_str: str):
    """Parse datetime string from CSV."""
    if not dt_str:
        return None
    
    # Try ISO format first
    try:
        return datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
    except ValueError:
        pass
    
    # Try common formats
    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%d.%m.%Y']:
        try:
            return datetime.strptime(dt_str, fmt)
        except ValueError:
            continue
    
    return None


def import_csv_to_db(conn, csv_path: str, batch_size: int = 100) -> int:
    """Import CSV data into database."""
    if not os.path.exists(csv_path):
        print(f"Error: CSV file not found: {csv_path}")
        return 0
    
    cursor = conn.cursor()
    records = []
    total_records = 0
    skipped_records = 0
    
    print(f"\nImporting data from: {csv_path}")
    
    with open(csv_path, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        
        for row in reader:
            try:
                # Parse values
                file_path = row.get('file_path', '')
                file_name = row.get('file_name', '')
                file_type = row.get('file_type', '') or None
                file_size = int(row.get('file_size', 0) or 0)
                content = row.get('content', '')
                archive_path = row.get('archive_path', '') or None
                created_date = parse_datetime(row.get('created_date', ''))
                content_hash = row.get('content_hash', '') or None
                
                # Skip if required fields are missing
                if not file_path or not file_name:
                    skipped_records += 1
                    continue
                
                records.append((
                    file_path,
                    file_name,
                    file_type,
                    file_size,
                    content,
                    archive_path,
                    created_date,
                    content_hash
                ))
                
                # Insert in batches
                if len(records) >= batch_size:
                    execute_batch(cursor, INSERT_DOCUMENT_SQL, records)
                    conn.commit()
                    total_records += len(records)
                    print(f"  Imported {total_records} records...")
                    records = []
            
            except Exception as e:
                print(f"  Warning: Skipping record due to error: {e}")
                skipped_records += 1
    
    # Insert remaining records
    if records:
        execute_batch(cursor, INSERT_DOCUMENT_SQL, records)
        conn.commit()
        total_records += len(records)
    
    print(f"\nImport complete!")
    print(f"  Total records imported: {total_records}")
    if skipped_records > 0:
        print(f"  Skipped records: {skipped_records}")
    
    return total_records


def verify_import(conn):
    """Verify imported data by running some queries."""
    cursor = conn.cursor()
    
    print("\n" + "=" * 60)
    print("Verifying import...")
    print("=" * 60)
    
    # Count total records
    cursor.execute("SELECT COUNT(*) FROM documents")
    total = cursor.fetchone()[0]
    print(f"Total documents in database: {total}")
    
    # Count by file type
    cursor.execute("""
        SELECT file_type, COUNT(*) 
        FROM documents 
        GROUP BY file_type 
        ORDER BY COUNT(*) DESC
    """)
    print("\nDocuments by type:")
    for row in cursor.fetchall():
        print(f"  {row[0] or 'Unknown'}: {row[1]}")
    
    # Show sample records
    cursor.execute("""
        SELECT id, file_name, file_type, LEFT(content, 50) AS preview
        FROM documents
        LIMIT 3
    """)
    print("\nSample records:")
    for row in cursor.fetchall():
        print(f"  ID {row[0]}: {row[1]} ({row[2]})")
        print(f"    Preview: {row[3]}...")
    
    # Test full-text search
    print("\nTesting full-text search...")
    cursor.execute("""
        SELECT file_name, ts_rank(search_vector, plainto_tsquery('russian', 'document')) AS rank
        FROM documents
        WHERE search_vector @@ plainto_tsquery('russian', 'document')
        ORDER BY rank DESC
        LIMIT 3
    """)
    results = cursor.fetchall()
    if results:
        print("  Search results for 'document':")
        for row in results:
            print(f"    - {row[0]} (rank: {row[1]:.4f})")
    else:
        print("  No results found for 'document'")


def main():
    """Main entry point for the import script."""
    parser = argparse.ArgumentParser(
        description='Import CSV data into PostgreSQL for full-text search'
    )
    parser.add_argument(
        '--csv', '-c',
        default='output/extracted_data.csv',
        help='Path to CSV file (default: output/extracted_data.csv)'
    )
    parser.add_argument(
        '--db', '-d',
        default='document_index',
        help='Database name (default: document_index)'
    )
    parser.add_argument(
        '--host', '-H',
        default='localhost',
        help='Database host (default: localhost)'
    )
    parser.add_argument(
        '--port', '-p',
        type=int,
        default=5432,
        help='Database port (default: 5432)'
    )
    parser.add_argument(
        '--user', '-u',
        default='postgres',
        help='Database user (default: postgres)'
    )
    parser.add_argument(
        '--password', '-P',
        default='postgres',
        help='Database password (default: postgres)'
    )
    parser.add_argument(
        '--setup-only',
        action='store_true',
        help='Only set up database schema, do not import data'
    )
    
    args = parser.parse_args()
    
    # Update database config
    db_config = {
        'host': args.host,
        'port': args.port,
        'database': args.db,
        'user': args.user,
        'password': args.password
    }
    
    print("=" * 60)
    print("Document Crawler Database Import")
    print("=" * 60)
    print(f"Database: {db_config['database']}@{db_config['host']}:{db_config['port']}")
    print(f"User: {db_config['user']}")

    # Create database if it doesn't exist
    if not create_database_if_not_exists(db_config):
        print("Failed to create database. Exiting.")
        return 1

    # Create connection
    conn = create_connection(db_config)
    if not conn:
        print("Failed to connect to database. Exiting.")
        return 1
    
    try:
        # Set up database schema
        setup_database(conn)
        
        if not args.setup_only:
            # Import CSV data
            imported = import_csv_to_db(conn, args.csv)
            
            if imported > 0:
                # Verify import
                verify_import(conn)
        
        print("\n" + "=" * 60)
        print("Import completed successfully!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nError during import: {e}")
        conn.rollback()
        return 1
    
    finally:
        conn.close()
        print("\nDatabase connection closed.")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())